{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"EvaluatingTKDMoves_v2.ipynb","provenance":[],"collapsed_sections":["pItoIBMJUAIj","voAnieeAs1sm","iSnMmPE_kiK3"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X38L6tanrnrB"},"source":["# Evaluating Taekwondo Moves\n","System to recognize and evaluate video of performed taekwondo moves"]},{"cell_type":"markdown","metadata":{"id":"NeW_EroTTwHg"},"source":["**Instructions**: Run all cells. When prompted to chose a file, import `VideoPose3D-2021-02-23.zip` and `IdealSet_NPYs&MP4s.zip`.\n"]},{"cell_type":"markdown","metadata":{"id":"pItoIBMJUAIj"},"source":["###Get Existing Custom Datasets as of February 23, 2021 & Ideal Set Videos"]},{"cell_type":"code","metadata":{"id":"pSv58-qSUA0a"},"source":["from google.colab import files\n","videopose3d = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"il-JhSA9UCd-"},"source":["unzip_folder(f'VideoPose3D-2021-02-23.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJmCXdi4UEeT"},"source":["ideals = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dzblhu2cUHDC"},"source":["unzip_folder('IdealSet_NPYs&MP4s.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voAnieeAs1sm"},"source":["##Setup"]},{"cell_type":"code","metadata":{"id":"MFffW6If8QSV"},"source":["import sys\n","import os\n","from os.path import exists, join, basename, splitext\n","import time\n","import math\n","import pickle\n","import numpy as np\n","import cv2\n","import json\n","import random\n","from google.colab.patches import cv2_imshow\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"hKb97qtlJCld"},"source":["!pip install -U scikit-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"nZ3Ud9zLgOoQ"},"source":["def show_local_mp4_video(file_name, width=640, height=480):\n","    import io\n","    import base64\n","    from IPython.display import HTML\n","    video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n","    return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n","                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n","                      </video>'''.format(width, height, video_encoded.decode('ascii')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"xa7Uzv5eDNoc"},"source":["def zip_folder(zip_folder_name, direc):\n","    \"\"\"\n","    Zips up the given folder\n","    :param zip_folder_name: the name for the zip folder, ex. `smedaram.zip`\n","    :param direc: the directory for which all the files inside are zipped\n","    \"\"\"\n","    !zip -q -r \"$zip_folder_name\" \"$direc\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0s_A1_9H0zCk"},"source":["def unzip_folder(zip_folder_name, direc='/content'):\n","    \"\"\"\n","    Unzips the given folder\n","    :param zip_folder_name: Unzips the given .zip file\n","    :param direc: the directory for where the .zip file is stored, assumed to be within /content, which is the base directory in Google Colab\n","    \"\"\"\n","    !unzip \"$zip_folder_name\" -d \"$direc\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAqUyekQ8q1c"},"source":["def remove_folder(folder_name):\n","    !rm -r \"$folder_name\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XyX3J_JOGM-"},"source":["def get_time():\n","    return f'{datetime.now()}'.replace(' ', '_').replace(':', '-')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2m1NUtAhEJpT"},"source":["#abbreviations for each of the 10 moves\n","abbreviations = {\n","    \"High block\": \"hb\",\n","    \"Middle block (in-to-out)\": \"mbio\",\n","    \"Middle block (out-to-in)\": \"mboi\",\n","    \"Knife hand block\": \"khb\",\n","    \"Low block\": \"lb\",\n","    \"Punch (middle-level)\": \"p\",\n","    \"High punch (face-level)\": \"hp\",\n","    \"Front kick\": \"fk\",\n","    \"Round kick\": \"rk\",\n","    \"Side kick\": \"sk\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuQOIhMI8hv4"},"source":["## Skeleton Representation"]},{"cell_type":"markdown","metadata":{"id":"H64yY_BJMX2h"},"source":["###Create Custom Datasets"]},{"cell_type":"code","metadata":{"id":"GsJSIwCD9hB3"},"source":["# git_repo_url = 'https://github.com/facebookresearch/VideoPose3D.git'\n","# project_name = splitext(basename(git_repo_url))[0]\n","# if not exists(project_name):\n","#   # clone and install dependencies\n","#    !git clone -q --depth 1 $git_repo_url"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TneKBqDy55Jk"},"source":["!python3 -m pip install --upgrade pip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-i4hmGYk1dL"},"source":["# install dependencies: \n","!pip install pyyaml==5.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version\n","# opencv is pre-installed on colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exqzM_yMVXxT"},"source":["# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","import torch\n","assert torch.__version__.startswith(\"1.8\")   # need to manually install torch 1.8 if Colab changes its default version\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n","# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyAvNCJMmvFF"},"source":["# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLuMRpbsTKLl"},"source":["def create_custom_dataset(leadup, ip_folder, slow):\n","    \"\"\"\n","    Creates custom dataset for all the files inside ip_folder\n","    :param leadup: the path to ip_folder\n","    :param ip_folder: the name of the folder which contains all video files for which the custom dataset should be made\n","    :param slow: boolean for whether or not the video should be slowed down before its processed frame by frame; currently obsolete, since discovered the slowing down the video does little to change a blurry frame in and of itself\n","    :return: the name of the output folder (where the .npy and .mp4 files will be stored) and the name of the custom dataset\n","    \"\"\"\n","    p = os.path.join(leadup, ip_folder)\n","    if slow == True:\n","        oup_folder = f'{ip_folder}_slow'\n","        new_p = os.path.join(leadup, oup_folder)\n","        if not os.path.exists(new_p):\n","          os.makedirs(new_p)\n","        \n","        for f in os.listdir(p):\n","            # print(f)\n","            vidfn, ext = os.path.splitext(f)\n","            slow_down_video(os.path.join(p, f), os.path.join(new_p, f), speed=2.0)\n","    else:\n","        new_p = p\n","        oup_folder = ip_folder\n","\n","    twod_outputs_folder = f'{oup_folder}_2d_outputs'\n","    if not os.path.exists(twod_outputs_folder):\n","        os.makedirs(twod_outputs_folder)\n","    !cd VideoPose3D/inference && python3 infer_video_d2.py --cfg COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml --output-dir ../../\"$twod_outputs_folder\"  --image-ext mp4 ../../\"$new_p\"/\n","    !cd VideoPose3D/data/ && python3 prepare_data_2d_custom.py -i ../../\"$twod_outputs_folder\" -o \"$oup_folder\"_dataset\n","    return oup_folder, f'{oup_folder}_dataset'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OPQyNKJs0SG0"},"source":["## Pose Classification"]},{"cell_type":"code","metadata":{"id":"7T0zHOUmUwGG"},"source":["def reconstruct_video_helper(path, threed_outputs_folder, custom_dataset, video_fn):\n","    \"\"\"\n","    Runs the command to reconstruct the given video into its 3D representation; creates a .npy (frame by frame position of joints) file and .mp4 video\n","    :param path: the path to the video\n","    :param threed_outputs_folder: the (path to the) folder where the output is stored\n","    :param custom_dataset: the name of the custom dataset which stores the 2D results from Detectron\n","    :param video_fn: the name of the video to be reconstructed\n","    \"\"\"\n","    \n","    p = os.path.join(path, video_fn)\n","    print(p)\n","    # print(f'cd VideoPose3D/ && python run.py -d custom -k {custom_dataset} -arc 3,3,3,3,3 -c checkpoint --evaluate pretrained_h36m_detectron_coco.bin --render --viz-subject {video_fn}.mp4 --viz-action custom --viz-camera 0 --viz-video ../{p}.mp4 --viz-output ../{threed_outputs_folder}/{video_fn}_si.mp4 --viz-export ../{threed_outputs_folder}/{video_fn}_si --viz-size 6')\n","    !cd VideoPose3D/ && python run.py -d custom -k \"$custom_dataset\" -arc 3,3,3,3,3 -c checkpoint --evaluate pretrained_h36m_detectron_coco.bin --render --viz-subject \"$video_fn\".mp4 --viz-action custom --viz-camera 0 --viz-video ../\"$p\".mp4 --viz-output ../\"$threed_outputs_folder\"/\"$video_fn\"_si.mp4 --viz-export ../\"$threed_outputs_folder\"/\"$video_fn\"_si --viz-size 6\n","\n","def reconstruct_videos(leadup, prefix, dts, folder=None):\n","    \"\"\"\n","    Runs `reconstruct_video_helper` for all the videos in the given folder\n","    :param leadup: the path to the folder\n","    :param prefix: if the videos are the training set, prefix is the name of the folder\n","    :param dts: the name of the custom dataset\n","    :param folder: if the videos are not part of the training set, this is the name of the folder\n","    :return: the name of the output folder\n","    \"\"\"\n","\n","    # path = f'../{prefix}/'\n","    # print(f'Dataset name: {dts}')\n","    if folder is None:\n","        p = os.path.join(leadup, prefix)\n","        threed_outputs_folder = os.path.join(leadup, f'{prefix}_3d_outputs')\n","    else:\n","        p = os.path.join(leadup, folder)\n","        threed_outputs_folder = os.path.join(leadup, f'{folder}_3d_outputs')\n","\n","    if not os.path.exists(threed_outputs_folder):\n","        os.makedirs(threed_outputs_folder)\n","\n","    for f in os.listdir(p):\n","        print(f)\n","        vidfn, ext = os.path.splitext(f)\n","        reconstruct_video_helper(p, threed_outputs_folder, dts, vidfn)\n","    return threed_outputs_folder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSnMmPE_kiK3"},"source":["###Generate Vocabulary Poses with K-means clustering"]},{"cell_type":"code","metadata":{"id":"JctIQCO0jFTa"},"source":["hand_techniques = [\"High block\", \"Middle block (in-to-out)\", \"Middle block (out-to-in)\", \"Knife hand block\", \"Low block\", \"Punch (middle-level)\", \"High punch (face-level)\"]\n","kicking_techniques = [\"Front kick\", \"Round kick\", \"Side kick\"]\n","\n","hand_vp = 10\n","kick_vp = 12\n","\n","ks_name = f'ks-h{hand_vp}k{kick_vp}.p'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGpX-sfIkhgc"},"source":["from sklearn.cluster import KMeans\n","\n","h36m_num_joints = 17\n","h36m_joints = {\n","    0: [7,8,9],\n","    1: [14,8,11],\n","    2: [8,14,15],\n","    3: [14,15,16],\n","    4: [8,11,12],\n","    5: [11,12,13],\n","    6: [7,1,2],\n","    7: [1,2,3],\n","    8: [7,4,5],\n","    9: [4,5,6],\n","    10: [16,7,13],\n","    11: [3,7,6],\n","    12: [8,7,16],\n","    13: [8,7,13]\n","}\n","def calc_angle(angle_jgroup, coordinates):\n","    \"\"\"\n","    Calculates all the angles as specified by angle_jgroup and coordinates\n","    :param angle_jgroup: a dictionary which references which 3 joints make which angle\n","    :param coordinates: a numpy array which represents all the x,y,z coordinates of the joints\n","    :return: a numpy array of angles between joints in radians\n","    \"\"\"\n","    angles = np.array([])\n","    for i in angle_jgroup:\n","        s,c,e = angle_jgroup[i]\n","        s_hat = coordinates[s] - coordinates[c]\n","        c_hat = coordinates[c] - coordinates[c]\n","        e_hat = coordinates[e] - coordinates[c]\n","\n","        dot_product = s_hat@e_hat\n","        alpha = math.acos((dot_product)/(np.linalg.norm(s_hat)*np.linalg.norm(e_hat)))\n","\n","        angles = np.append(angles, alpha)\n","    return angles\n","\n","def get_angles(data):\n","    \"\"\"\n","    Calculates angles between joints for each frame in a given video, represented as `data`\n","    :param data: the .npy file containing a numpy array of coordinates of joints for each frame in video\n","    :return: all the angles, a series of numpy arrays (each array is one frame)\n","    \"\"\"\n","    ang = np.empty((len(data), len(h36m_joints)))\n","    for i, frame in enumerate(data):\n","        f_ang = ang = calc_angle(h36m_joints, frame)\n","        ang[i] = f_ang\n","    return ang"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xy6dU4Zv3B6r"},"source":["def get_cluster_centroids(k, ips):\n","    \"\"\"\n","    Returns `k` cluster centroids for videos represented by `ips`, a bunch of NumPy archives\n","    :param k: number of clusters\n","    :param ips: all the input files, a list of .npy files\n","    :return: scikit-learn's kmeans object, which contains .cluster_centers_\n","    \"\"\"\n","    X = get_angles(np.load(ips[0]))\n","    i = 1\n","    while i < len(ips):\n","        data = get_angles(np.load(ips[i]))\n","        X = np.concatenate((X, data))\n","        i += 1\n","\n","    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n","\n","    # print(kmeans.labels_)\n","    # cc = kmeans.cluster_centers_\n","    # print(cc)\n","    return kmeans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPO3lOxBL73l"},"source":["def get_ips(leadup, prefix):\n","    \"\"\"\n","    Gets all the input filenames, as in all the 3D reconstruction .npy files within the folder\n","    :param leadup: the path to the folder\n","    :param prefix: the name of the folder (most often the prefix, if its the training set)\n","    :return: the list of all input filenames (including paths)\n","    \"\"\"\n","    threed_outputs_folder = os.path.join(leadup, f'{prefix}_3d_outputs')\n","    ips = []\n","    for f in os.listdir(threed_outputs_folder):\n","        vidfn, ext = os.path.splitext(f)\n","        # print(ext)\n","        if ext == '.npy':\n","            ips.append(os.path.join(threed_outputs_folder, f))\n","    print(ips)\n","    return ips"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ev2IZn2hG3jn"},"source":["###KNN"]},{"cell_type":"code","metadata":{"id":"fg2KI702H-4Q"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","def classify_videos(ips, tech, ks_name, new_ids=False):\n","    \"\"\"\n","    Classifies all videos in `ips` according to whether its a hand or foot technique and to the kmeans objects containing vocabulary poses into one of the 20 possibilities (10 moves, each side)\n","    :param ips: list of strings for the names of all .npy files that should be classified into a technique\n","    :param tech: list of strings, either 'h' for hand technique or 'k' for kicking technique, that match up with the list of filenames in `ips` #obsolete in sample runs since we just assume its the technique which requires the largest number of vocabulary poses (so kicking techniques)\n","    :param ks_name: the name of the pickle file where the kmeans objects from training are stored (containing the vocabulary poses for each of the 20 possibilities), ex. ks-h10k12.p\n","    :param new_ids: boolean for whether or not new identifiers should be generated, assumed to already be stored in `moveids.p`\n","    :return: an array of tuples, the first value is the string abbreviation for which move the given video has been classified into and a string for how many out of the total vocabulary poses led to that conclusion\n","    \"\"\"\n","    path_to_ks = os.path.join(lu, ks_name)\n","    suffix = path_to_ks[path_to_ks.find('-')+1:path_to_ks.find('.p')]\n","\n","    if not os.path.exists(os.path.join(lu, f'{suffix}.npz')):\n","        X, y, identifier2move, move2identifier = get_data(path_to_ks, suffix, new_identifiers=new_ids)\n","    else:\n","        npzfile = np.load(os.path.join(lu, f'{suffix}.npz'))\n","        X,y = npzfile['arr_0'], npzfile['arr_1']\n","        identifier2move, move2identifier = pickle.load(open(os.path.join(lu, f'moveids.p'), 'rb'))\n","\n","    # neigh = KNeighborsClassifier(n_neighbors=5)\n","    neigh = KNeighborsClassifier(n_neighbors=10)\n","    # neigh = KNeighborsClassifier(n_neighbors=12)\n","    neigh.fit(X, y)\n","\n","    final_res = []\n","    for i, ip in enumerate(ips):\n","        if tech == None or tech[i] == 'k':\n","          num_vp = int(suffix[suffix.find('k')+1:])\n","        elif tech[i] == 'h':\n","            num_vp = int(suffix[suffix.find('h')+1:suffix.find('k')])\n","\n","        sample_kmeans = get_cluster_centroids(num_vp, [ip])\n","\n","        res = []\n","        for pose in sample_kmeans.cluster_centers_:\n","            res.append(neigh.predict([pose]))\n","\n","        tally = {}\n","        for r in res:\n","            m = identifier2move[r[0]]\n","            if m not in tally:\n","                tally[m] = 1\n","            else:\n","                tally[m] += 1\n","\n","        # for w in sorted(tally, key=tally.get, reverse=True):\n","        #     print(w, tally[w])\n","            # final_res.append(tally.values())\n","        maxvote_move = max(tally, key=tally.get)\n","        final_res.append((maxvote_move, f'{tally[maxvote_move]}/{sum(tally.values())}'))\n","    return final_res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wW3L_AfM0Wum"},"source":["## Temporal Alignment (DTW)"]},{"cell_type":"code","metadata":{"id":"XaCS0cRjW3kk"},"source":["from scipy.spatial.distance import euclidean\n","from fastdtw import fastdtw\n","def dtw_compare_samples(ips, oup):\n","    \"\"\"\n","    Compares all the videos represented by .npy files in `ips` to the output `oup` using fastdtw\n","    :param ips: the list of strings of the names of input .npy files\n","    :param oup: the path to the ideal .npy file for this given move\n","    :return: the distance (similarity) between the ip and oup\n","    \"\"\"\n","    d = []\n","    y = get_angles(np.load(oup))\n","    for i in ips:\n","        X = get_angles(np.load(i))\n","        distance, path = fastdtw(X, y, dist=euclidean)\n","        d.append(distance)\n","    return d\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvmDHyhoVX1g"},"source":["git_repo_url = 'https://github.com/SriMed/sr_eval_tkd_v2.git'\n","project_name = splitext(basename(git_repo_url))[0]\n","if not exists(project_name):\n","  # clone and install dependencies\n","   !git clone -q --depth 1 $git_repo_url"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv6OgVX3VYVg"},"source":["lu = 'sr_eval_tkd_v2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o5mle3sf3fd"},"source":["!chmod u+x \"$lu\"/convert2mp4.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwdDr2fNdoou"},"source":["def convert2mp4(base):\n","  !cd \"$base\" && ./../\"$lu\"/convert2mp4.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7WWHp4vrfWTd"},"source":["from IPython.display import clear_output\n","def evaluate_video():\n","    \"\"\"\n","    Given the name of a video, \n","    1) ensures its in the necesssary directories and that it's a .mp4 file\n","    2) Creates a custom dataset\n","    3) Reconstructs the video\n","    4) Classifies the video\n","    5) Temporally aligns with DTW\n","    6) Prints the results\n","    \"\"\"\n","    file = input('Enter the name of your video (ex. i/am/a/boss/test.mp4): ')  # assume in directory\n","    lu_pipe, tail = os.path.split(file)\n","    vidfn, ext = os.path.splitext(tail)\n","\n","    if ext != \".mp4\":\n","        convert2mp4(lu_pipe)\n","\n","    ip_f_path = os.path.join(lu_pipe, vidfn)\n","    if not os.path.exists(ip_f_path):\n","        os.makedirs(ip_f_path)\n","    !mv\n","    \"$file\" \"$ip_f_path\"\n","\n","    oup_f, dts = create_custom_dataset(lu_pipe, vidfn, slow=False)\n","    threed_outputs_folder_path = reconstruct_videos(lu_pipe, None, dts, oup_f)\n","    show_local_mp4_video(f'{threed_outputs_folder_path}/{vidfn}_si.mp4')\n","    ips = get_ips(lu_pipe, oup_f)\n","    res = classify_videos(ips, None, ks_name)\n","    move = res[0][0]\n","    confidence = res[0][1]\n","    confidence = int(confidence[:confidence.find('/')]) / int(confidence[confidence.find('/') + 1:])\n","\n","    clear_output()\n","\n","    print(f\"Your video was classified as a {move} with {confidence * 100}% confidence\")\n","    oup = os.path.join('ideal_3d_outputs', f'{move}_ideal_si.npy')\n","    d = dtw_compare_samples(ips, oup)\n","    print(f\"Relative to the exemplar, your score is {d}. The closer to 0, the better your execution. Try again to get closer to 0.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94stPmRbmxtf"},"source":["evaluate_video()"],"execution_count":null,"outputs":[]}]}